<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F27%2Fpandas%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[[TOC] 基础 Series数据类型numpy的一维数组(array)和对应的一个索引组成,也类似于python数据类型里的字典1234567891011121314151617181920#列表、字典和Series的转化s1 = pd.Series([1,3,5,7],index=[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;])s2=pd.Series(&#123;&quot;a&quot;:4,&quot;b&quot;:2,&quot;c&quot;:1,&quot;d&quot;:8&#125;)s1 +s2 # Series可以自动对齐,使得相同索引的值进行相加li = list(s1) # [1, 3, 5, 7]dic = dict(s1) #&#123;&apos;a&apos;: 1, &apos;b&apos;: 3, &apos;c&apos;: 5, &apos;d&apos;: 7&#125;# array和Series转化a1 = np.random.randint(0,10,(5,)) # 这是个一维数组 array类型# array([8, 5, 3, 4, 5])s1 = pd.Series(a1)索引 值0 81 92 43 44 9np.array(s1) # 转化会numpy类型# array([8, 9, 4, 4, 9]) 方法12345678910111213141516%%time# 统计一个月内站子微博内容包含 公益 或者 应援的 个数data = []for i in db.starcomm.find().sort([(&quot;_id&quot;,1)]): ret = &#123;&#125; ret[&quot;name&quot;] = i.get(&quot;name&quot;) ret[&quot;sum&quot;] = db[i.get(&quot;table&quot;)].find(&#123;&quot;publish_time&quot;: &#123;&quot;$gt&quot;: &quot;2019-06-00&quot;,&quot;$lt&quot;:&quot;2019-07-01&quot;&#125;&#125;,&#123;&quot;_id&quot;:0,&quot;content&quot;:1&#125;).count() res = db[i.get(&quot;table&quot;)].find(&#123;&quot;publish_time&quot;: &#123;&quot;$gt&quot;: &quot;2019-06-00&quot;,&quot;$lt&quot;:&quot;2019-07-01&quot;&#125;&#125;,&#123;&quot;_id&quot;:0,&quot;content&quot;:1&#125;) df = pd.DataFrame(res) # df.content.str.contains(&quot;公益&quot;) 得到的是个bool的Series ret[&quot;gongyi&quot;] = df[df.content.str.contains(&quot;公益&quot;)][&quot;content&quot;].count() # 也可匹配正则 ret[&quot;yingyuan&quot;] = df[df.content.str.contains(&quot;应援&quot;)][&quot;content&quot;].count() print(ret) data.append(ret)df = pd.DataFrame(data)df.to_excel(&quot;站子统计2.xlsx&quot;,index=0) 12345# pandas 转化为excel参数,是否包含索引,选择的列,给列重新起名字df.to_excel(f&quot;spider/download/&#123;table&#125;-&#123;name&#125;.xlsx&quot;.format(table=table, name=name), index=False, columns=[&quot;user&quot;, &quot;uid&quot;, &quot;mid&quot;, &quot;publish_time&quot;, &quot;content&quot;, &quot;forward&quot;, &quot;comment&quot;, &quot;like&quot;], header=[&quot;姓名&quot;, &quot;id&quot;,&quot;微博id&quot;, &quot;发布时间&quot;,&quot;内容&quot;, &quot;转发数&quot;,&quot;评论数&quot;, &quot;点赞数&quot; ] ) 12345678910import pandas as pddf1 = pd.read_excel(&quot;姚弛.xlsx&quot;,sheet_name=0)df2 = pd.read_excel(&quot;姚弛去重后.xlsx&quot;,sheet_name=0)# 写入同一个excel的多个工作簿write = pd.ExcelWriter(&quot;t2.xlsx&quot;)df1.to_excel(write,&quot;a&quot;)df2.to_excel(write,&quot;b&quot;)write.save()df = pd.read_excel(&quot;test.xls&quot;,sheet_name=1)# sheet_name 选择工作簿,可以根据索引选择,也可以根据名字选择 对 excel数据的两个字段进行排序,使用openpyxl修改pandas导出后的excel 123456789101112131415161718192021res = list( db[table].find(&#123;&quot;star_name&quot;: &#123;&quot;$regex&quot;: name&#125;&#125;, &#123;&quot;_id&quot;: 0&#125;).sort([(&quot;_id&quot;, -1)]).limit(1000))name = name.replace(&quot;.*&quot;, &quot;all&quot;)df = pd.DataFrame(res)df = df[~df[&quot;type2&quot;].isna()] # 去除没有分类的空值, ~ 取反,isna()是否为空df[&quot;followers_count&quot;] = df[&quot;followers_count&quot;].astype(int)df.sort_values(by=&quot;followers_count&quot;, axis=0, ascending=False, inplace=True, kind=&apos;mergesort&apos;) # 使用桶排,保留原有顺序,ascending=False降序,inplace=True替换现有表格df[&quot;type_sorted&quot;] = df[&quot;type2&quot;].apply( lambda i: 1 if i == &quot;个人KOL&quot; or i == &quot;个人kol&quot; or i == &quot;kol&quot; else 0) # 新建一个字段用来排序df.sort_values(by=&quot;type_sorted&quot;, axis=0, ascending=True, inplace=True, kind=&apos;mergesort&apos;, )df.to_excel(f&quot;spider/download/&#123;table&#125;-&#123;name&#125;.xlsx&quot;.format(table=table, name=name), index=False, startrow=1, merge_cells=True, columns=[&quot;screen_name&quot;, &quot;gender&quot;, &quot;followers_count&quot;, &quot;type1&quot;, &quot;type2&quot;], header=[&quot;昵称&quot;, &quot;性别&quot;, &quot;粉丝数&quot;, &quot;大分类&quot;, &quot;小分类&quot;], )workbook = openpyxl.load_workbook(f&quot;spider/download/&#123;table&#125;-&#123;name&#125;.xlsx&quot;.format(table=table, name=name))worksheet = workbook.worksheets[0]worksheet.merge_cells(&apos;A1:E1&apos;)worksheet.cell(1, 1, f&apos;&#123;name&#125;站子及kol数据&apos;)workbook.save(filename=f&quot;spider/download/&#123;table&#125;-&#123;name&#125;.xlsx&quot;.format(table=table, name=name)) 按类别排序 123list_sorted = [&quot;综合站&quot;,&quot;综合&quot;,&quot;后援会组&quot;,&quot;反黑站&quot;,&quot;反黑&quot;,&quot;控评组&quot;,&quot;公益站&quot;,&quot;公益&quot;,&quot;应援站&quot;,&quot;应援&quot;,&quot;法援站&quot;,&quot;数据站&quot;,&quot;数据&quot;,&quot;打投组&quot;,&quot;打投&quot;,&quot;产出站&quot;,&quot;产出&quot;,&quot;时尚站&quot;,&quot;时尚&quot;,&quot;图站&quot;,&quot;图&quot;,&quot;资源博&quot;,&quot;地区粉丝团&quot;,&quot;个人kol&quot;,&quot;kol&quot;,&quot;个人KOL&quot;,]df[&apos;type2&apos;] = df[&apos;type2&apos;].astype(&apos;category&apos;).cat.set_categories(list_sorted)df = df.sort_values(by=[&apos;type2&apos;], ascending=True) 更改索引 123df.reset_index(drop=True,inplace=True,) # 重置索引,从0开始df[&quot;序号&quot;] = np.arange(1, len(df)+1) # 增加序号字段,从1开始df[&quot;序号&quot;]= df.index +1 # 也可重置索引后使用这种方法增加序号字段,从1开始 取上一行的数据 1234567891011121314#这一行数据1&gt;数据2 AND 数据1的上一行&lt;数据2的上一行#例如上例子，6&gt;3 AND 4&lt;5 则输出 产品C#shift(0):当前行#shift(1):前一行#shift(n):往前第n行df = pd.DataFrame(&#123;&apos;产品&apos;: [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;], &apos;数据1&apos;: [1, 4, 6], &apos;数据2&apos;: [2, 5, 3]&#125;) 产品 数据1 数据20 A 1 21 B 4 52 C 6 3df[(df[&apos;数据1&apos;].shift(1) &lt; df[&apos;数据2&apos;].shift(1)) &amp; (df[&apos;数据1&apos;].shift(0) &gt; df[&apos;数据2&apos;].shift(0))][&apos;产品&apos;] 123456789101112131415161718192021222324252627282930313233import numpy as npimport pandas as pd df = pd.DataFrame(&#123;&apos;date&apos;: [&apos;20150101&apos;,&apos;20150102&apos;,&apos;20150103&apos;,&apos;20150104&apos;,&apos;20150105&apos;,&apos;20150106&apos;], &apos;A&apos;: [8,10,9,11,11,12], &apos;B&apos;: [7,9,8,11,10,10], &apos;M&apos;: [7.5,9.5,8.5,11,10.5,11], &apos;S&apos;: [0,-1,1,0,0,-1]&#125;)df = df.reindex(columns=[&apos;date&apos;,&apos;A&apos;,&apos;B&apos;,&apos;M&apos;,&apos;S&apos;]) # 重建索引df date A B M S0 20150101 8 7 7.5 01 20150102 10 9 9.5 -12 20150103 9 8 8.5 13 20150104 11 11 11.0 04 20150105 11 10 10.5 05 20150106 12 10 11.0 -1# if S &lt; 0, cost = (M-B).shift(1)*S# if S &gt; 0, cost = (M-A).shift(1)*S# if S == 0, cost=0方法一:df[&apos;cost&apos;] = np.where(df[&apos;S&apos;] &lt; 0, np.roll((df[&apos;M&apos;]-df[&apos;B&apos;]), 1)*df[&apos;S&apos;], np.where(df[&apos;S&apos;] &gt; 0, np.roll((df[&apos;M&apos;]-df[&apos;A&apos;]), 1)*df[&apos;S&apos;], 0) )方法二:M, A, B, S = [df[col] for col in &apos;MABS&apos;]conditions = [S &lt; 0, S &gt; 0]choices = [(M-B).shift(1)*S, (M-A).shift(1)*S]df[&apos;cost2&apos;] = np.select(conditions, choices, default=0) 分片,统计数量 12345678910111213141516171819202122232425262728import numpy as npimport pandas as pdvalues = np.random.rand(10)valuesarray([0.87058577, 0.48406958, 0.28367199, 0.61107191, 0.85048026, 0.53127231, 0.14198227, 0.33746115, 0.47544542, 0.00764242])bins = [0,0.2,0.4,0.6,0.8,1]pd.get_dummies(pd.cut(values,bins))(0.0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1.0]0 0 0 0 0 11 0 0 1 0 02 0 1 0 0 03 0 0 0 1 04 0 0 0 0 15 0 0 1 0 06 1 0 0 0 07 0 1 0 0 08 0 0 1 0 09 1 0 0 0 0pd.get_dummies(pd.cut(values,bins)).sum()(0.0, 0.2] 2(0.2, 0.4] 2(0.4, 0.6] 3(0.6, 0.8] 1(0.8, 1.0] 2dtype: int64 1234567891011121314151617181920212223242526272829303132333435363738394041import jsondb = json.load(open(&quot;database.json&quot;))nutrients = pd.DataFrame(db[0][&quot;nutrients&quot;])info_keys = [&quot;description&quot;,&quot;group&quot;,&quot;id&quot;,&quot;manufacturer&quot;]info = pd.DataFrame(db,columns=info_keys)pd.value_counts(info.group)nutrients = []for rec in db: fnuts = pd.DataFrame(rec[&quot;nutrients&quot;]) fnuts[&quot;id&quot;] = rec[&quot;id&quot;] nutrients.append(fnuts)nutrients = pd.concat(nutrients,ignore_index=True)nutrients.duplicated().sum() # 重复值的个数nutrients = nutrients.drop_duplicates() #去重col_mapping = &#123; &quot;description&quot; :&quot;food&quot;, &quot;group&quot;:&quot;fgroup&quot;&#125;info = info.rename(columns=col_mapping,copy=False) # 重命名col_mapping = &#123; &quot;description&quot;:&quot;nutrient&quot;, &quot;group&quot;:&quot;nutgroup&quot;&#125;nutrients = nutrients.rename(columns=col_mapping,copy=False)ndata = pd.merge(nutrients,info,on=&quot;id&quot;,how=&quot;outer&quot;) # 合并ndata.ix[3] # loc：通过行标签索引数据 ; iloc：通过行号索引行数据 ; ix：通过行标签或行号索引数据（基于loc和iloc的混合）result= ndata.groupby([&quot;nutrient&quot;,&quot;fgroup&quot;])[&quot;value&quot;].quantile(0.5) # 分位数by_nutrient = ndata.groupby([&quot;nutgroup&quot;,&quot;nutrient&quot;])get_maximum = lambda x:x.xs(x.value.idxmax()) # xs 按行索引的名字选择行 ; idxmax 方法用于获取 Series 的最大值的索引值get_minimum = lambda x:x.xs(x.value.idxmin())max_foods = by_nutrient.apply(get_maximum)[[&quot;value&quot;,&quot;food&quot;]]max_foods.food = max_foods.food.str[:50] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134df = pd.DataFrame(&#123;&quot;key1&quot;:[&quot;a&quot;,&quot;a&quot;,&apos;b&apos;,&apos;b&apos;,&apos;a&apos;],&quot;key2&quot;:[&quot;one&quot;,&quot;two&quot;,&quot;one&quot;,&quot;two&quot;,&quot;one&quot;],&quot;data1&quot;:np.random.randn(5),&quot;data2&quot;:np.random.randn(5)&#125;)dfkey1 key2 data1 data20 a one 0.040481 1.4208741 a two 1.158794 -1.5342562 b one -0.701728 0.9332593 b two -0.538727 0.5037274 a one 1.245693 -1.553059grouped = df[&apos;data1&apos;].groupby(df[&quot;key1&quot;])grouped.mean()key1a 0.814990b -0.620228Name: data1, dtype: float64means = df[&quot;data1&quot;].groupby([df[&quot;key1&quot;],df[&quot;key2&quot;]]).mean()meanskey1 key2a one 0.643087 two 1.158794b one -0.701728 two -0.538727Name: data1, dtype: float64means.unstack()key2 one twokey1 a 0.643087 1.158794b -0.701728 -0.538727means.unstack(&quot;key1&quot;)key1 a bkey2 one 0.643087 -0.701728two 1.158794 -0.538727dfkey1 key2 data1 data20 a one 0.040481 1.4208741 a two 1.158794 -1.5342562 b one -0.701728 0.9332593 b two -0.538727 0.5037274 a one 1.245693 -1.553059states = np.array([&apos;Ohio&apos;,&apos;California&apos;,&apos;California&apos;,&apos;Ohio&apos;,&apos;Ohio&apos;])years = np.array([2005,2005,2006,2005,2005])df[&apos;data1&apos;].groupby([states,years]).mean()California 2005 1.158794 2006 -0.701728Ohio 2005 0.249149Name: data1, dtype: float64df.groupby(&apos;key1&apos;).mean()data1 data2key1 a 0.814990 -0.555480b -0.620228 0.718493df.groupby([&apos;key1&apos;,&apos;key2&apos;]).size()key1 key2a one 2 two 1b one 1 two 1dtype: int64for name,group in df.groupby(&quot;key1&quot;)[&quot;key2&quot;]: print(name) print(group)a0 one1 two4 oneName: key2, dtype: objectb2 one3 twoName: key2, dtype: objectfor (k1,k2),group in df.groupby([&quot;key1&quot;,&quot;key2&quot;]): print(k1,k2) print(group)a one key1 key2 data1 data20 a one 0.040481 1.4208744 a one 1.245693 -1.553059a two key1 key2 data1 data21 a two 1.158794 -1.534256b one key1 key2 data1 data22 b one -0.701728 0.933259b two key1 key2 data1 data23 b two -0.538727 0.503727pieces = dict(list(df.groupby(&apos;key1&apos;))) # 变成字典,取分组后切片pieces&#123;&apos;a&apos;: key1 key2 data1 data2 0 a one 0.040481 1.420874 1 a two 1.158794 -1.534256 4 a one 1.245693 -1.553059, &apos;b&apos;: key1 key2 data1 data2 2 b one -0.701728 0.933259 3 b two -0.538727 0.503727&#125;pieces[&quot;a&quot;]key1 key2 data1 data20 a one 0.040481 1.4208741 a two 1.158794 -1.5342564 a one 1.245693 -1.553059# 按列分组df.dtypeskey1 objectkey2 objectdata1 float64data2 float64dtype: objectgrouped = df.groupby(df.dtypes,axis=1)dict(list(grouped))&#123;dtype(&apos;float64&apos;): data1 data2 0 0.040481 1.420874 1 1.158794 -1.534256 2 -0.701728 0.933259 3 -0.538727 0.503727 4 1.245693 -1.553059, dtype(&apos;O&apos;): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one&#125; 12345678910111213141516171819202122232425262728293031323334353637dfkey1 key2 data1 data20 a one 0.014271 0.9952021 a two -1.214673 0.7717522 b one 0.963552 -0.6106593 b two -1.238167 -0.7794694 a one -0.161038 0.297883for i in df.groupby(&apos;key1&apos;)[&quot;data2&quot;]: print(i)(&apos;a&apos;, 0 0.9952021 0.7717524 0.297883Name: data2, dtype: float64)(&apos;b&apos;, 2 -0.6106593 -0.779469Name: data2, dtype: float64)for i in df.groupby(&apos;key1&apos;)[[&quot;data2&quot;]]: print(i)(&apos;a&apos;, key1 key2 data1 data20 a one 0.014271 0.9952021 a two -1.214673 0.7717524 a one -0.161038 0.297883)(&apos;b&apos;, key1 key2 data1 data22 b one 0.963552 -0.6106593 b two -1.238167 -0.779469)for i in df[[&quot;data2&quot;]].groupby(df[&apos;key1&apos;]): print(i)(&apos;a&apos;, data20 0.9952021 0.7717524 0.297883)(&apos;b&apos;, data22 -0.6106593 -0.779469) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788people = pd.DataFrame(np.random.randn(5,5),columns=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;],index=[&apos;Joe&apos;,&apos;Steve&apos;,&apos;Wes&apos;,&apos;Jim&apos;,&apos;Travis&apos;])peoplea b c d eJoe -0.168996 1.395623 0.554432 1.782690 0.434736Steve -1.527405 -0.368840 0.788034 1.393361 0.106949Wes -0.938562 0.974203 -0.764173 -0.120886 -1.032622Jim -0.033004 0.050498 0.286138 0.990956 -0.863128Travis -0.764984 -0.058035 -3.228946 -2.364380 0.083468people.ix[2:3,[&apos;b&apos;,&apos;c&apos;]] = np.nan # 行索引2:3,列索引&apos;b&apos;,&apos;c&apos;设为nanpeoplea b c d eJoe -0.168996 1.395623 0.554432 1.782690 0.434736Steve -1.527405 -0.368840 0.788034 1.393361 0.106949Wes -0.938562 NaN NaN -0.120886 -1.032622Jim -0.033004 0.050498 0.286138 0.990956 -0.863128Travis -0.764984 -0.058035 -3.228946 -2.364380 0.083468# 以字典对组,将多个列对应一个组mapping = &#123;&apos;a&apos;:&apos;red&apos;,&apos;b&apos;:&apos;red&apos;,&apos;c&apos;:&apos;blue&apos;,&apos;d&apos;:&apos;blue&apos;,&apos;e&apos;:&apos;red&apos;,&apos;f&apos;:&apos;orange&apos;&#125;by_column = people.groupby(mapping,axis=1)by_column.sum()blue redJoe 2.337122 1.661363Steve 2.181395 -1.789296Wes -0.120886 -1.971185Jim 1.277094 -0.845634Travis -5.593326 -0.739551# 使用Series类型分组map_servies = pd.Series(mapping)map_serviesa redb redc blued bluee redf orangedtype: objectpeople.groupby(map_servies,axis=1).count() blue redJoe 2 3Steve 2 3Wes 1 2Jim 2 3Travis 2 3# 通过函数进行分组,更加灵活people.groupby(len).sum() # 通过索引名长度对行进行分组a b c d e3 -1.140563 1.446122 0.840571 2.652759 -1.4610155 -1.527405 -0.368840 0.788034 1.393361 0.1069496 -0.764984 -0.058035 -3.228946 -2.364380 0.083468# 混合使用key_list = [&apos;one&apos;,&apos;one&apos;,&apos;one&apos;,&apos;two&apos;,&apos;two&apos;]people.groupby([len,key_list]).min()a b c d e3 one -0.938562 1.395623 0.554432 -0.120886 -1.032622two -0.033004 0.050498 0.286138 0.990956 -0.8631285 one -1.527405 -0.368840 0.788034 1.393361 0.1069496 two -0.764984 -0.058035 -3.228946 -2.364380 0.083468# 根据索引级别分组columns= pd.MultiIndex.from_arrays([[&apos;US&apos;,&apos;US&apos;,&apos;US&apos;,&apos;JP&apos;,&apos;JP&apos;],[1,3,5,1,3]],names=[&apos;cty&apos;,&apos;tenor&apos;])hier_df = pd.DataFrame(np.random.randn(4,5),columns=columns)hier_df cty US JPtenor 1 3 5 1 30 0.097867 0.344498 0.319310 1.501781 -2.0501671 0.194749 -0.859610 1.357505 -2.789579 0.0964792 0.844357 -0.804864 -1.269237 0.158462 -0.1269333 -0.327278 1.108394 -1.627974 0.252040 0.614040hier_df.groupby(level=&apos;cty&apos;,axis=1).count()cty JP US0 2 31 2 32 2 33 2 3hier_df.groupby(level=&apos;cty&apos;,axis=1).first()cty JP US0 1.501781 0.0978671 -2.789579 0.1947492 0.158462 0.8443573 0.252040 -0.327278 123456789101112131415161718192021222324df key1 key2 data1 data20 a one -0.178398 0.8096461 a two -0.855168 0.1242302 b one -0.389141 -0.5896443 b two 0.142801 -0.3361994 a one 2.965736 0.252593grouped = df.groupby(&apos;key1&apos;)dict(list(grouped))&#123;&apos;a&apos;: key1 key2 data1 data2 0 a one -1.347478 -0.582278 1 a two -0.188355 1.044322 4 a one -0.427903 -0.182019, &apos;b&apos;: key1 key2 data1 data2 2 b one 0.656312 -0.391641 3 b two -0.114511 1.015607&#125;def peak_to_peak(arr): return [round(i,2) for i in list(arr)]grouped.agg(peak_to_peak).loc[&apos;a&apos;,[&quot;data1&quot;]]data1 [-1.35, -0.19, -0.43]Name: a, dtype: object 12345678910查看每个客户应答为true后应答为false不多于2个的客户for name,group in df.groupby(&apos;客户号码&apos;): group.sort_values(by=&quot;呼出时间&quot;, axis=0, ascending=False, inplace=True, kind=&apos;mergesort&apos;) group.reset_index(drop=True,inplace=True,) df2 = group[group[&quot;应答&quot;]] if not df2.empty and (group.tail(1).index -df2.tail(1).index)&gt;2: print(group.tail(1)) break 12忽略警告设置pd.set_option(&apos;mode.chained_assignment&apos;, None) 1234567891011121314151617from datetime import datetime, timedeltabaidu = pd.DataFrame(db.baiduheat.find(&#123;&quot;name&quot;: name&#125;, &#123;&quot;_id&quot;: 0&#125;).limit(31).sort([(&quot;_id&quot;, -1)]))df1 = pd.DataFrame(db.weixinheat.find(&#123;&quot;name&quot;: name&#125;, &#123;&quot;_id&quot;: 0&#125;).sort([(&quot;_id&quot;, -1)]).limit(1))weibo = pd.DataFrame(db.weiheat.find(&#123;&quot;name&quot;: name&#125;,&#123;&quot;_id&quot;: 0&#125;).sort([(&quot;_id&quot;, -1)]).limit(31))df2 = pd.concat([pd.Series(row[&apos;weixin_index&apos;].split(&apos;,&apos;)) for _, row in df1.iterrows()]).reset_index()weixinheat = list(df2[0])weixinheat.reverse()df2[&quot;weixinheat&quot;] = weixinheatdf2[&quot;date&quot;] = [(datetime.strptime(df1[&quot;date&quot;][0], &quot;%Y-%m-%d&quot;) - timedelta(days=i)).strftime(&apos;%Y-%m-%d&apos;) for i in range(len(df2))]all = pd.merge(pd.merge(baidu, weibo, on=&quot;date&quot;, how=&quot;outer&quot;), df2, on=&quot;date&quot;, how=&quot;outer&quot;)final_all = all.head(30)final_all.drop([&quot;name_x&quot;, &quot;name_y&quot;, 0, &quot;media&quot;, &quot;uid&quot;, &quot;index&quot;], axis=1, inplace=True) //删除不要的字段final_all.rename(index=str, inplace=True, columns=&#123;&quot;date&quot;: &quot;日期&quot;, &quot;message&quot;: &quot;资讯指数&quot;, &quot;search&quot;: &quot;搜索指数&quot;, &quot;weiheat&quot;: &quot;微博指数&quot;, &quot;weixinheat&quot;: &quot;微信指数&quot;&#125;) //重命名res = json.loads(final_all.to_json(orient=&apos;records&apos;)) //dataframe转化为json]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo-use]]></title>
    <url>%2F2019%2F09%2F27%2Fhexo-use%2F</url>
    <content type="text"><![CDATA[新建文章 12hexo new hello-world创建的文章会出现在 source/_posts 文件夹下，是 MarkDown 格式 标签页 123456789101112131415161718192021现在我们的博客只有首页、文章页，如果我们想要增加标签页，可以自行添加，这里 Hexo 也给我们提供了这个功能，在根目录执行命令如下：hexo new page tags执行这个命令之后会自动帮我们生成一个 source/tags/index.md 文件，内容就只有这样子的：---title: tagsdate: 2019-09-26 16:44:17---我们可以自行添加一个 type 字段来指定页面的类型：type: tagscomments: false然后再在主题的 _config.yml 文件将这个页面的链接添加到主菜单里面，修改 menu 字段如下menu: home: / || home #about: /about/ || user tags: /tags/ || tags #categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat然后重启项目 分类页 12345678910111213141516分类功能和标签类似，一个文章可以对应某个分类，如果要增加分类页面可以使用如下命令创建hexo new page categories然后同样地，会生成一个 source/categories/index.md 文件。我们可以自行添加一个 type 字段来指定页面的类型：type: categoriescomments: false然后再在主题的 _config.yml 文件将这个页面的链接添加到主菜单里面，修改 menu 字段如下menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
